{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softmax.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vassilybar/cs231n_assignments_colab/blob/master/assignment_1/softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w29dBOnyKbf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Softmax exercise\n",
        "\n",
        "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
        "\n",
        "This exercise is analogous to the SVM exercise. You will:\n",
        "\n",
        "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** with numerical gradient\n",
        "- use a validation set to **tune the learning rate and regularization** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "metadata": {
        "id": "XTXSTL3IKdfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1241
        },
        "outputId": "31ffb712-7540-42bb-d270-2f8bb82ec24b"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf * && \\\n",
        " wget 'http://cs231n.github.io/assignments/2018/spring1718_assignment1.zip' && \\\n",
        " unzip 'spring1718_assignment1.zip' && \\\n",
        " mv assignment1/cs231n . && \\\n",
        " pushd cs231n/datasets && \\\n",
        " ./get_datasets.sh && \\\n",
        " popd && \\\n",
        " rm -rf assignment1 && \\\n",
        " rm -rf cs231n/__pycache__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-15 19:42:52--  http://cs231n.github.io/assignments/2018/spring1718_assignment1.zip\n",
            "Resolving cs231n.github.io (cs231n.github.io)... 185.199.110.153, 185.199.108.153, 185.199.109.153, ...\n",
            "Connecting to cs231n.github.io (cs231n.github.io)|185.199.110.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73256 (72K) [application/zip]\n",
            "Saving to: ‘spring1718_assignment1.zip’\n",
            "\n",
            "\r          spring171   0%[                    ]       0  --.-KB/s               \rspring1718_assignme 100%[===================>]  71.54K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-01-15 19:42:52 (2.43 MB/s) - ‘spring1718_assignment1.zip’ saved [73256/73256]\n",
            "\n",
            "Archive:  spring1718_assignment1.zip\n",
            "   creating: assignment1/\n",
            " extracting: assignment1/.gitignore  \n",
            "   creating: assignment1/.ipynb_checkpoints/\n",
            "  inflating: assignment1/.ipynb_checkpoints/features-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/knn-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/softmax-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/svm-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/two_layer_net-checkpoint.ipynb  \n",
            "  inflating: assignment1/collectSubmission.sh  \n",
            "   creating: assignment1/cs231n/\n",
            "   creating: assignment1/cs231n/classifiers/\n",
            "  inflating: assignment1/cs231n/classifiers/k_nearest_neighbor.py  \n",
            "  inflating: assignment1/cs231n/classifiers/linear_classifier.py  \n",
            "  inflating: assignment1/cs231n/classifiers/linear_svm.py  \n",
            "  inflating: assignment1/cs231n/classifiers/neural_net.py  \n",
            "  inflating: assignment1/cs231n/classifiers/softmax.py  \n",
            "  inflating: assignment1/cs231n/classifiers/__init__.py  \n",
            "   creating: assignment1/cs231n/datasets/\n",
            "  inflating: assignment1/cs231n/datasets/.gitignore  \n",
            "  inflating: assignment1/cs231n/datasets/get_datasets.sh  \n",
            "  inflating: assignment1/cs231n/data_utils.py  \n",
            "  inflating: assignment1/cs231n/features.py  \n",
            "  inflating: assignment1/cs231n/gradient_check.py  \n",
            "  inflating: assignment1/cs231n/vis_utils.py  \n",
            " extracting: assignment1/cs231n/__init__.py  \n",
            "   creating: assignment1/cs231n/__pycache__/\n",
            "  inflating: assignment1/cs231n/__pycache__/data_utils.cpython-36.pyc  \n",
            "  inflating: assignment1/cs231n/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: assignment1/features.ipynb  \n",
            "  inflating: assignment1/frameworkpython  \n",
            "  inflating: assignment1/knn.ipynb   \n",
            "  inflating: assignment1/README.md   \n",
            "  inflating: assignment1/requirements.txt  \n",
            "  inflating: assignment1/setup_googlecloud.sh  \n",
            "  inflating: assignment1/softmax.ipynb  \n",
            "  inflating: assignment1/start_ipython_osx.sh  \n",
            "  inflating: assignment1/svm.ipynb   \n",
            "  inflating: assignment1/two_layer_net.ipynb  \n",
            "/content/cs231n/datasets /content\n",
            "--2019-01-15 19:42:52--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  2.24MB/s    in 41s     \n",
            "\n",
            "2019-01-15 19:43:34 (3.92 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fI_5cfPxKbf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from cs231n.data_utils import load_CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a2dFnX0lKbgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "56017005-2455-4897-9cf9-89c6be8c2b70"
      },
      "cell_type": "code",
      "source": [
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier. These are the same steps as we used for the\n",
        "    SVM, but condensed to a single function.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "    \n",
        "    # subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "    X_dev = X_train[mask]\n",
        "    y_dev = y_train[mask]\n",
        "    \n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "    \n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis = 0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "    X_dev -= mean_image\n",
        "    \n",
        "    # add bias dimension and transform into columns\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
        "\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "print('dev labels shape: ', y_dev.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 3073)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3073)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 3073)\n",
            "Test labels shape:  (1000,)\n",
            "dev data shape:  (500, 3073)\n",
            "dev labels shape:  (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WBp3OHAiKbgR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Softmax Classifier\n",
        "\n",
        "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
      ]
    },
    {
      "metadata": {
        "id": "8UmRt0j3K46m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "def softmax_loss_naive(W, X, y, reg):\n",
        "  \"\"\"\n",
        "  Softmax loss function, naive implementation (with loops)\n",
        "\n",
        "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "  of N examples.\n",
        "\n",
        "  Inputs:\n",
        "  - W: A numpy array of shape (D, C) containing weights.\n",
        "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "    that X[i] has label c, where 0 <= c < C.\n",
        "  - reg: (float) regularization strength\n",
        "\n",
        "  Returns a tuple of:\n",
        "  - loss as single float\n",
        "  - gradient with respect to weights W; an array of same shape as W\n",
        "  \"\"\"\n",
        "  # Initialize the loss and gradient to zero.\n",
        "  loss = 0.0\n",
        "  dW = np.zeros_like(W)\n",
        "  num_classes = W.shape[1]\n",
        "  num_train = X.shape[0]\n",
        "  \n",
        "  #############################################################################\n",
        "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
        "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "  # regularization!                                                           #\n",
        "  #############################################################################\n",
        "  \n",
        "  for i in range(num_train):\n",
        "    scores = X[i].dot(W)\n",
        "    scores -= np.max(scores)\n",
        "    prob = np.exp(scores) / np.sum(np.exp(scores))\n",
        "    loss -= np.log(prob[y[i]])\n",
        "    prob[y[i]] -= 1\n",
        "    for j in range(num_classes):\n",
        "      dW[:,j] += prob[j] * X[i]   \n",
        "    \n",
        "  loss /= num_train\n",
        "  loss += reg * np.sum(W * W)\n",
        "  dW /= num_train\n",
        "  dW += 2 * reg * W\n",
        "   \n",
        "  #############################################################################\n",
        "  #                          END OF YOUR CODE                                 #\n",
        "  #############################################################################\n",
        "\n",
        "  return loss, dW\n",
        "\n",
        "\n",
        "def softmax_loss_vectorized(W, X, y, reg):\n",
        "  \"\"\"\n",
        "  Softmax loss function, vectorized version.\n",
        "\n",
        "  Inputs and outputs are the same as softmax_loss_naive.\n",
        "  \"\"\"\n",
        "  # Initialize the loss and gradient to zero.\n",
        "  loss = 0.0\n",
        "  dW = np.zeros_like(W)\n",
        "  num_train = X.shape[0]\n",
        "\n",
        "  #############################################################################\n",
        "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "  # regularization!                                                           #\n",
        "  #############################################################################\n",
        "  \n",
        "  scores = X.dot(W)\n",
        "  scores -= np.max(scores, axis=1).reshape(-1,1)\n",
        "  scores = np.exp(scores)\n",
        "  probs = scores / np.sum(scores, axis=1).reshape(-1,1)\n",
        "  loss = - np.sum(np.log(probs[range(num_train), y]))\n",
        "  probs[range(num_train), y] -= 1\n",
        "  dW = X.T.dot(probs)\n",
        "  \n",
        "  loss /= num_train\n",
        "  loss += reg * np.sum(W * W)\n",
        "  dW /= num_train\n",
        "  dW += 2 * reg * W\n",
        "  \n",
        "  #############################################################################\n",
        "  #                          END OF YOUR CODE                                 #\n",
        "  #############################################################################\n",
        "\n",
        "  return loss, dW\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Iz9KUg5KbgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0aa1c242-f14d-4b41-95c6-bbc07b504005"
      },
      "cell_type": "code",
      "source": [
        "# First implement the naive softmax loss function with nested loops.\n",
        "# Open the file cs231n/classifiers/softmax.py and implement the\n",
        "# softmax_loss_naive function.\n",
        "\n",
        "# from cs231n.classifiers.softmax import softmax_loss_naive\n",
        "import time\n",
        "\n",
        "# Generate a random softmax weight matrix and use it to compute the loss.\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
        "print('loss: %f' % loss)\n",
        "print('sanity check: %f' % (-np.log(0.1)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 2.304856\n",
            "sanity check: 2.302585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EIIxtDvcKbgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inline Question 1:\n",
        "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
        "\n",
        "**Your answer:** *Fill this in*\n"
      ]
    },
    {
      "metadata": {
        "id": "PcwQqacUKbgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "cf528aa0-3b57-4064-9739-bf1abf01302e"
      },
      "cell_type": "code",
      "source": [
        "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
        "# version of the gradient that uses nested loops.\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
        "# The numeric gradient should be close to the analytic gradient.\n",
        "from cs231n.gradient_check import grad_check_sparse\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
        "\n",
        "# similar to SVM case, do another gradient check with regularization\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: 0.472231 analytic: 0.472231, relative error: 3.657419e-08\n",
            "numerical: 1.111719 analytic: 1.111719, relative error: 2.572296e-08\n",
            "numerical: 0.104712 analytic: 0.104712, relative error: 2.878868e-08\n",
            "numerical: -0.874898 analytic: -0.874898, relative error: 7.292852e-09\n",
            "numerical: -0.707584 analytic: -0.707584, relative error: 2.893817e-08\n",
            "numerical: -0.745993 analytic: -0.745993, relative error: 1.188909e-07\n",
            "numerical: -1.372103 analytic: -1.372104, relative error: 6.777983e-08\n",
            "numerical: -1.207652 analytic: -1.207652, relative error: 5.611830e-09\n",
            "numerical: -1.282066 analytic: -1.282066, relative error: 8.041017e-09\n",
            "numerical: 0.854724 analytic: 0.854724, relative error: 1.435545e-08\n",
            "numerical: 2.077817 analytic: 2.077817, relative error: 2.473374e-08\n",
            "numerical: 2.350816 analytic: 2.350816, relative error: 4.457307e-09\n",
            "numerical: -3.337176 analytic: -3.337176, relative error: 2.075839e-09\n",
            "numerical: 2.122853 analytic: 2.122853, relative error: 1.277980e-09\n",
            "numerical: -1.923398 analytic: -1.923398, relative error: 1.142646e-08\n",
            "numerical: -1.437858 analytic: -1.437858, relative error: 5.741708e-09\n",
            "numerical: 0.275016 analytic: 0.275016, relative error: 1.066570e-08\n",
            "numerical: -2.220048 analytic: -2.220048, relative error: 2.328509e-08\n",
            "numerical: -1.031573 analytic: -1.031573, relative error: 8.220602e-09\n",
            "numerical: 0.293630 analytic: 0.293630, relative error: 8.196202e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6FbwwODxKbgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4ab77731-bee9-402d-edc2-0a7fd15157ec"
      },
      "cell_type": "code",
      "source": [
        "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
        "# implement a vectorized version in softmax_loss_vectorized.\n",
        "# The two versions should compute the same results, but the vectorized version should be\n",
        "# much faster.\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "# from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
        "# of the gradient.\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
        "print('Gradient difference: %f' % grad_difference)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive loss: 2.304856e+00 computed in 0.097190s\n",
            "vectorized loss: 2.304856e+00 computed in 0.013757s\n",
            "Loss difference: 0.000000\n",
            "Gradient difference: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rykWzXACMbaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "# from cs231n.classifiers.softmax import *\n",
        "\n",
        "class LinearClassifier(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.W = None\n",
        "\n",
        "  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    \"\"\"\n",
        "    Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "      training samples each of dimension D.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "      means that X[i] has label 0 <= c < C for C classes.\n",
        "    - learning_rate: (float) learning rate for optimization.\n",
        "    - reg: (float) regularization strength.\n",
        "    - num_iters: (integer) number of steps to take when optimizing\n",
        "    - batch_size: (integer) number of training examples to use at each step.\n",
        "    - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "    Outputs:\n",
        "    A list containing the value of the loss function at each training iteration.\n",
        "    \"\"\"\n",
        "    num_train, dim = X.shape\n",
        "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "    if self.W is None:\n",
        "      # lazily initialize W\n",
        "      self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "    # Run stochastic gradient descent to optimize W\n",
        "    loss_history = []\n",
        "    for it in range(num_iters):\n",
        "      X_batch = None\n",
        "      y_batch = None\n",
        "\n",
        "      #########################################################################\n",
        "      # TODO:                                                                 #\n",
        "      # Sample batch_size elements from the training data and their           #\n",
        "      # corresponding labels to use in this round of gradient descent.        #\n",
        "      # Store the data in X_batch and their corresponding labels in           #\n",
        "      # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "      # and y_batch should have shape (batch_size,)                           #\n",
        "      #                                                                       #\n",
        "      # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "      # replacement is faster than sampling without replacement.              #\n",
        "      #########################################################################\n",
        "      \n",
        "      idxs = np.random.choice(num_train, batch_size)\n",
        "      X_batch = X[idxs]\n",
        "      y_batch = y[idxs]\n",
        "      \n",
        "      #########################################################################\n",
        "      #                       END OF YOUR CODE                                #\n",
        "      #########################################################################\n",
        "\n",
        "      # evaluate loss and gradient\n",
        "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      # perform parameter update\n",
        "      #########################################################################\n",
        "      # TODO:                                                                 #\n",
        "      # Update the weights using the gradient and the learning rate.          #\n",
        "      #########################################################################\n",
        "      \n",
        "      self.W += - learning_rate * grad\n",
        "      \n",
        "      #########################################################################\n",
        "      #                       END OF YOUR CODE                                #\n",
        "      #########################################################################\n",
        "\n",
        "      if verbose and it % 100 == 0:\n",
        "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Use the trained weights of this linear classifier to predict labels for\n",
        "    data points.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "      training samples each of dimension D.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "      array of length N, and each element is an integer giving the predicted\n",
        "      class.\n",
        "    \"\"\"\n",
        "    y_pred = np.zeros(X.shape[0])\n",
        "    ###########################################################################\n",
        "    # TODO:                                                                   #\n",
        "    # Implement this method. Store the predicted labels in y_pred.            #\n",
        "    ###########################################################################\n",
        "    \n",
        "    y_pred = np.argmax(X.dot(self.W), axis=1)\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                           END OF YOUR CODE                              #\n",
        "    ###########################################################################\n",
        "    return y_pred\n",
        "  \n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    \"\"\"\n",
        "    Compute the loss function and its derivative. \n",
        "    Subclasses will override this.\n",
        "\n",
        "    Inputs:\n",
        "    - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "      data points; each point has dimension D.\n",
        "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "    - reg: (float) regularization strength.\n",
        "\n",
        "    Returns: A tuple containing:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to self.W; an array of the same shape as W\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class Softmax(LinearClassifier):\n",
        "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RtoZRhIxKbgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "54992981-8a34-4233-f497-26e9f47e5ec9"
      },
      "cell_type": "code",
      "source": [
        "# Use the validation set to tune hyperparameters (regularization strength and\n",
        "# learning rate). You should experiment with different ranges for the learning\n",
        "# rates and regularization strengths; if you are careful you should be able to\n",
        "# get a classification accuracy of over 0.35 on the validation set.\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "learning_rates = [1e-7, 5e-7]\n",
        "regularization_strengths = [2.5e4, 5e4]\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for reg in regularization_strengths:\n",
        "        sftmx = Softmax()\n",
        "        sftmx.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=1000)\n",
        "        y_train_pred = sftmx.predict(X_train)\n",
        "        y_val_pred = sftmx.predict(X_val)\n",
        "        train_accuracy = np.mean(y_train == y_train_pred)\n",
        "        val_accuracy = np.mean(y_val == y_val_pred)\n",
        "        if val_accuracy > best_val:\n",
        "            best_val = val_accuracy\n",
        "            best_softmax = sftmx\n",
        "        results[(lr, reg)] = (train_accuracy, val_accuracy)\n",
        "\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    \n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.329224 val accuracy: 0.345000\n",
            "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.309102 val accuracy: 0.320000\n",
            "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.330245 val accuracy: 0.348000\n",
            "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.307163 val accuracy: 0.311000\n",
            "best validation accuracy achieved during cross-validation: 0.348000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7wT3ZgYHKbg5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71df41a0-d991-43d1-b090-78bd9f51997b"
      },
      "cell_type": "code",
      "source": [
        "# evaluate on test set\n",
        "# Evaluate the best softmax on test set\n",
        "y_test_pred = best_softmax.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax on raw pixels final test set accuracy: 0.330000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X9GYc-j6KbhE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Inline Question** - *True or False*\n",
        "\n",
        "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
        "\n",
        "*Your answer*:\n",
        "\n",
        "*Your explanation*:"
      ]
    },
    {
      "metadata": {
        "id": "bd99-OYEKbhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "644a65b0-e4ad-4a54-d013-71f48399aed9"
      },
      "cell_type": "code",
      "source": [
        "# Visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAF7CAYAAAAkBgR2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8deb4824a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Im5vO8r7Eo6S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}